{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spooky_author.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "dNM_8M7dowRc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Spooky Author Classification\n",
        "\n",
        "This notebook demonstrates how to download data using the Kaggle API, create word embeddings, and build a CNN for the [Spooky Author Identification Challenge](https://www.kaggle.com/c/spooky-author-identification).  \n",
        "\n",
        "\n",
        "\n",
        "##Downloading the Data using the Kaggle API\n",
        "\n",
        "Install the Kaggle library to allow you to use the Kaggle API.  Download your credentials, \"kaggle.json\", from the Kaggle website.  The following instructions to download your kaggle credentials to your local computer:\n",
        "\n",
        "\n",
        "\n",
        "1.   Log in to Kaggle.com.  This should take you to home hub.\n",
        "2.   Click the picture of your profile on the top right corner of your home hub.  Select My Account.\n",
        "3.   In your account settings, find API and click on the Create New API Token button.  This should automatically download your credentials to your computer.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "58KrB2npo8KT",
        "colab_type": "code",
        "outputId": "0724b121-582a-4122-fbb9-0d229adca77a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "cell_type": "code",
      "source": [
        "#install Kaggle library\n",
        "!pip install Kaggle"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Kaggle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/9b/ac57e15fbb239c6793c8d0b7dfd1a4c4a025eaa9f791b5388a7afb515aed/kaggle-1.5.0.tar.gz (53kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.23.0,>=1.15 in /usr/local/lib/python3.6/dist-packages (from Kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from Kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from Kaggle) (2018.10.15)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from Kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from Kaggle) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from Kaggle) (4.28.1)\n",
            "Collecting python-slugify (from Kaggle)\n",
            "  Downloading https://files.pythonhosted.org/packages/00/ad/c778a6df614b6217c30fe80045b365bfa08b5dd3cb02e8b37a6d25126781/python-slugify-1.2.6.tar.gz\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->Kaggle) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->Kaggle) (3.0.4)\n",
            "Collecting Unidecode>=0.04.16 (from python-slugify->Kaggle)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/ef/67085e30e8bbcdd76e2f0a4ad8151c13a2c5bce77c85f8cad6e1f16fb141/Unidecode-1.0.22-py2.py3-none-any.whl (235kB)\n",
            "\u001b[K    100% |████████████████████████████████| 235kB 5.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Kaggle, python-slugify\n",
            "  Running setup.py bdist_wheel for Kaggle ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8b/21/3b/a0076243c6ae12a6215b2da515fe06b539aee7217b406e510e\n",
            "  Running setup.py bdist_wheel for python-slugify ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e3/65/da/2045deea3098ed7471eca0e2460cfbd3fdfe8c1d6fa6fcac92\n",
            "Successfully built Kaggle python-slugify\n",
            "Installing collected packages: Unidecode, python-slugify, Kaggle\n",
            "Successfully installed Kaggle-1.5.0 Unidecode-1.0.22 python-slugify-1.2.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2EgfdK4fX_Ic",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After running the following code, follow the link to receive a verification code.  Copy the code and paste it in the textbox that appears.  Ensure that the credential is located in the correct folder.  If your credential is placed in your google drive or google cloud storage, ensure that the credential is located in the correct folder.  In the following code below, I need to create a folder in my google drive called \".kaggle\" and place my credential in that folder.\n",
        "\n",
        "```\n",
        "filename = \"/root/.kaggle/kaggle.json\"\n",
        "```\n"
      ]
    },
    {
      "metadata": {
        "id": "m6QQ_yKdY9oe",
        "colab_type": "code",
        "outputId": "30d14c34-a4e1-45e0-8462-71f68c0f0b6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "import io, os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(\n",
        "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "filename = \"/root/.kaggle/kaggle.json\"\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "fh = io.FileIO(filename, 'wb')\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "os.chmod(filename, 600)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download 100%.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cGyyeNQ2YEm6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Download the files from the Spooky Author Identification Challenge using the Kaggle API.  An easy way to get the correct command is to go to the [data page of the competition](https://www.kaggle.com/c/spooky-author-identification/data) and copy the command located to the right of the data sources."
      ]
    },
    {
      "metadata": {
        "id": "Q9F06_tEaYvm",
        "colab_type": "code",
        "outputId": "a126f225-3c20-4768-9366-1f93421f6c7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c spooky-author-identification"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading sample_submission.zip to /content\n",
            "\r  0% 0.00/29.4k [00:00<?, ?B/s]\n",
            "100% 29.4k/29.4k [00:00<00:00, 17.1MB/s]\n",
            "Downloading test.zip to /content\n",
            "  0% 0.00/538k [00:00<?, ?B/s]\n",
            "100% 538k/538k [00:00<00:00, 76.7MB/s]\n",
            "Downloading train.zip to /content\n",
            "  0% 0.00/1.26M [00:00<?, ?B/s]\n",
            "100% 1.26M/1.26M [00:00<00:00, 133MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nzZVPrvMvs54",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Unzip the downloaded files.  "
      ]
    },
    {
      "metadata": {
        "id": "VoBMEwcMFSem",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "currentPath = os.getcwd()\n",
        "\n",
        "\n",
        "zip_ref = zipfile.ZipFile(currentPath + \"/train.zip\", 'r')\n",
        "zip_ref.extractall(currentPath)\n",
        "zip_ref = zipfile.ZipFile(currentPath + \"/test.zip\", 'r')\n",
        "zip_ref.extractall(currentPath)\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q8ywogBfcmy5",
        "colab_type": "code",
        "outputId": "1f99855a-806c-40ee-ddcb-2ce44cf45952",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"train.csv\")\n",
        "train.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>id26305</td>\n",
              "      <td>This process, however, afforded me no means of...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>id17569</td>\n",
              "      <td>It never once occurred to me that the fumbling...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>id11008</td>\n",
              "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
              "      <td>EAP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>id27763</td>\n",
              "      <td>How lovely is spring As we looked from Windsor...</td>\n",
              "      <td>MWS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>id12958</td>\n",
              "      <td>Finding nothing else, not even gold, the Super...</td>\n",
              "      <td>HPL</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id                                               text author\n",
              "0  id26305  This process, however, afforded me no means of...    EAP\n",
              "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
              "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
              "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
              "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "BO25Vz2en05F",
        "colab_type": "code",
        "outputId": "07a0a0e1-51ac-4a07-adf6-7d0d70fbd0c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(train)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19579"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "SjdxtCcByVMa",
        "colab_type": "code",
        "outputId": "652f2531-1ed2-4ae5-9d8a-9fe4dd982721",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "lines = train['text'].tolist()\n",
        "lines[0:5]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.',\n",
              " 'It never once occurred to me that the fumbling might be a mere mistake.',\n",
              " 'In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.',\n",
              " 'How lovely is spring As we looked from Windsor Terrace on the sixteen fertile counties spread beneath, speckled by happy cottages and wealthier towns, all looked as in former years, heart cheering and fair.',\n",
              " 'Finding nothing else, not even gold, the Superintendent abandoned his attempts; but a perplexed look occasionally steals over his countenance as he sits thinking at his desk.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "571KwSgUoeMN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Create a Vocabulary\n",
        "\n",
        "The following code maps each word to a unique number.\n",
        "\n",
        "Excerpts that contain more words than the maximum document length will be truncated.  Excerpts that contain fewer words than the maximum document length will be padded with multiple instances of the PADWORD.  A PADWORD is a word that is not expected to be found in any excerpt.  All excerpts will end up containing 200 words.  The following code will produce the \"vocab.tsv\" file which will mapped each unique word with a number.\n"
      ]
    },
    {
      "metadata": {
        "id": "93oOspAytxma",
        "colab_type": "code",
        "outputId": "7ca77023-70c8-4d91-98d7-630c724b8ff5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.learn as tflearn\n",
        "import tensorflow.contrib.layers as tflayers\n",
        "from tensorflow.contrib.learn.python.learn import learn_runner\n",
        "import tensorflow.contrib.metrics as metrics\n",
        "from tensorflow.python.platform import gfile\n",
        "from tensorflow.contrib import lookup\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "# variables set by init()\n",
        "BUCKET = None\n",
        "TRAIN_STEPS = 1000\n",
        "WORD_VOCAB_FILE = None \n",
        "N_WORDS = -1\n",
        "\n",
        "# hardcoded into graph\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# describe your data\n",
        "TARGETS = ['EAP', 'HPL', 'MWS']\n",
        "MAX_DOCUMENT_LENGTH = 200\n",
        "CSV_COLUMNS = ['text', 'author']\n",
        "LABEL_COLUMN = 'author'\n",
        "DEFAULTS = [['null'], ['null']]\n",
        "PADWORD = 'ASDFG'\n",
        "\n",
        "# create vocabulary\n",
        "vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH)\n",
        "vocab_processor.fit(lines)\n",
        "with gfile.Open('vocab.tsv', 'wb') as f:\n",
        "    f.write(\"{}\\n\".format(PADWORD))\n",
        "    for word, index in vocab_processor.vocabulary_._mapping.items():\n",
        "      f.write(\"{}\\n\".format(word))\n",
        "N_WORDS = len(vocab_processor.vocabulary_)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-8-2835e45a8639>:34: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tensorflow/transform or tf.data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wnCgi3ckABti",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The 'vocab.tsv' file will contain two columns.  The second column represents the word and the first column represents the corresponding number associated to the word.  The first two words in the dictionary are the pad word (with a corresponding value of 0) and UNK (with a corresponding value of 1) which represents words found in test data that were not found in the training data."
      ]
    },
    {
      "metadata": {
        "id": "UWgLGnQnzcRs",
        "colab_type": "code",
        "outputId": "ba839965-ea31-4f26-ae0e-270653d449d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "cell_type": "code",
      "source": [
        "checkVocab = pd.read_csv(\"vocab.tsv\", header = None)\n",
        "checkVocab.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ASDFG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;UNK&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>process</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>however</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0\n",
              "0    ASDFG\n",
              "1    <UNK>\n",
              "2     This\n",
              "3  process\n",
              "4  however"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "l20kb3CqKzrQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following code peforms a lookup for the words \"process\" and \"left\".  The word \"process\" corresponds with the number 3 and the word \"left\" corresponds with the number 49."
      ]
    },
    {
      "metadata": {
        "id": "Qrw6UBzD3X1q",
        "colab_type": "code",
        "outputId": "d9941274-f981-4d20-b9cb-579aee996b51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "table = lookup.index_table_from_file(\n",
        "  vocabulary_file='vocab.tsv', num_oov_buckets=1, vocab_size=None, default_value=-1)\n",
        "numbers = table.lookup(tf.constant('process left'.split()))\n",
        "with tf.Session() as sess:\n",
        "  tf.tables_initializer().run()\n",
        "  print (\"{} --> {}\".format(lines[0], numbers.eval()))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall. --> [ 3 49]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KK0732UCNN9c",
        "colab_type": "code",
        "outputId": "00eb374c-c090-43d0-c384-c8c3d030cec1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(checkVocab)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28361"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "3m-9kEma229U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Process Words\n",
        "\n",
        "Create a tensor for each excerpt in the training data and each word.  The words tensor below will create a mapping between each word and the word's index.  The word's index is represented by a vector of size 2.  The first value in the vector corresponds to the excerpt index and the second value corresponds to the word's index within the excerpt."
      ]
    },
    {
      "metadata": {
        "id": "xTrTvMLMVOUl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# string operations\n",
        "excerpts = tf.constant(lines)\n",
        "words = tf.string_split(lines)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CwwCDh5PmC8r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create a tensor that contains  vectors of strings where each vector represents the excerpt and each string is either a word in the excerpt or the padword.  The words in the excerpt should be in order based on the original data.  The padwords are added at the end of the words.  The tensor called numbers should be a numerical representation of densewords where each word in the excerpt is replaced with the corresponding number in the vocabulary."
      ]
    },
    {
      "metadata": {
        "id": "RHnq12J-VWye",
        "colab_type": "code",
        "outputId": "77489d3e-f31d-4117-aec2-51892536e81d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "densewords = tf.sparse.to_dense(words, default_value=PADWORD)\n",
        "numbers = table.lookup(densewords)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a4KFud4Inaj9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "qPvi23t1b1q8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "padding = tf.constant([[0,0],[0,MAX_DOCUMENT_LENGTH]])\n",
        "padded = tf.pad(numbers, padding)\n",
        "sliced = tf.slice(padded, [0,0], [-1, MAX_DOCUMENT_LENGTH])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8m5dz_XEHbsF",
        "colab_type": "code",
        "outputId": "c94d7c5d-3afa-43b2-db25-2a17eb7e4bfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "sliced"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'Slice:0' shape=(19579, 200) dtype=int64>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "oCK0s8OlFPJv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FiQvslBZFPe5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#just dimensionality reduction - -no context words\n",
        "EMBEDDING_SIZE = 10\n",
        "embeds = tf.contrib.layers.embed_sequence(sliced, \n",
        "                 vocab_size=N_WORDS, embed_dim=EMBEDDING_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y6-TPyk-Fo63",
        "colab_type": "code",
        "outputId": "e6dd3225-0760-4252-dac8-6c477a8cc8c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "embeds"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'EmbedSequence/embedding_lookup/Identity:0' shape=(19579, 200, 10) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "b-BqFCEuFivU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "WINDOW_SIZE = EMBEDDING_SIZE\n",
        "STRIDE = int(WINDOW_SIZE/2)\n",
        "conv = tf.contrib.layers.conv1d(embeds, 1, WINDOW_SIZE, \n",
        "                stride=STRIDE, padding='SAME') # (?, 4, 1)    \n",
        "conv = tf.nn.relu(conv) # (?, 4, 1)    \n",
        "words = tf.squeeze(conv, [2]) # (?, 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l9FWMy8SHDcd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HBil_dt-Hb0l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# CNN model parameters\n",
        "EMBEDDING_SIZE = 10\n",
        "WINDOW_SIZE = EMBEDDING_SIZE\n",
        "STRIDE = int(WINDOW_SIZE/2)\n",
        "def cnn_model(features, label, mode):\n",
        "    \n",
        "    table = lookup.index_table_from_file(\n",
        "      vocabulary_file='vocab.tsv', num_oov_buckets=1, vocab_size=None, default_value=-1)\n",
        "    numbers = table.lookup(tf.constant('process left'.split()))\n",
        "    \n",
        "    # make targets numeric\n",
        "    mapping_strings = tf.constant(TARGETS)\n",
        "    table_labels = tf.contrib.lookup.index_table_from_tensor(\n",
        "       mapping=mapping_strings, num_oov_buckets=1, default_value=-1)\n",
        "    label_transformed = tf.constant(label)\n",
        "    target = table_labels.lookup(label_transformed)\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    # string operations\n",
        "    lines = features['text'].tolist()\n",
        "    excerpts = tf.constant(lines)\n",
        "    words = tf.string_split(lines)\n",
        "    densewords = tf.sparse_tensor_to_dense(words, default_value=PADWORD)\n",
        "    numbers = table.lookup(densewords)\n",
        "    padding = tf.constant([[0,0],[0,MAX_DOCUMENT_LENGTH]])\n",
        "    padded = tf.pad(numbers, padding)\n",
        "    sliced = tf.slice(padded, [0,0], [-1, MAX_DOCUMENT_LENGTH])\n",
        "    print('words_sliced={}'.format(words))  # (?, 20)\n",
        "\n",
        "    # layer to take the words and convert them into vectors (embeddings)\n",
        "    embeds = tf.contrib.layers.embed_sequence(sliced, vocab_size=N_WORDS, embed_dim=EMBEDDING_SIZE)\n",
        "    print('words_embed={}'.format(embeds)) # (?, 20, 10)\n",
        "    \n",
        "    # now do convolution\n",
        "    conv = tf.contrib.layers.conv1d(embeds, 1, WINDOW_SIZE, stride=STRIDE, padding='SAME') # (?, 4, 1)\n",
        "    conv = tf.nn.relu(conv) # (?, 4, 1)\n",
        "    words = tf.squeeze(conv, [2]) # (?, 4)\n",
        "    print('words_conv={}'.format(words)) # (?, 4)\n",
        "\n",
        "    n_classes = len(TARGETS)\n",
        "\n",
        "    logits = tf.contrib.layers.fully_connected(words, n_classes, activation_fn=None)\n",
        "    print('logits={}'.format(logits)) # (?, 3)\n",
        "    predictions_dict = {\n",
        "      'author': tf.gather(TARGETS, tf.argmax(logits, 1)),\n",
        "      'class': tf.argmax(logits, 1),\n",
        "      'prob': tf.nn.softmax(logits)\n",
        "    }\n",
        "\n",
        "    if mode == tf.contrib.learn.ModeKeys.TRAIN or mode == tf.contrib.learn.ModeKeys.EVAL:\n",
        "       loss = tf.losses.sparse_softmax_cross_entropy(target, logits)\n",
        "       train_op = tf.contrib.layers.optimize_loss(\n",
        "         loss,\n",
        "         tf.contrib.framework.get_global_step(),\n",
        "         optimizer='Adam',\n",
        "         learning_rate=0.01)\n",
        "    else:\n",
        "       loss = None\n",
        "       train_op = None\n",
        "\n",
        "    return tflearn.ModelFnOps(\n",
        "      mode=mode,\n",
        "      predictions=predictions_dict,\n",
        "      loss=loss,\n",
        "      train_op=train_op)\n",
        "\n",
        "\n",
        "def serving_input_fn():\n",
        "    feature_placeholders = {\n",
        "      'text': tf.placeholder(tf.string, [None]),\n",
        "    }\n",
        "    features = {\n",
        "      key: tf.expand_dims(tensor, -1)\n",
        "      for key, tensor in feature_placeholders.items()\n",
        "    }\n",
        "    return tflearn.utils.input_fn_utils.InputFnOps(\n",
        "      features,\n",
        "      None,\n",
        "      feature_placeholders)\n",
        "\n",
        "\n",
        "from tensorflow.contrib.learn.python.learn.utils import saved_model_export_utils\n",
        "def experiment_fn(output_dir):\n",
        "    # run experiment\n",
        "    return tflearn.Experiment(\n",
        "        tflearn.Estimator(model_fn=cnn_model, model_dir=output_dir),\n",
        "        train_input_fn=get_train(),\n",
        "        eval_input_fn=get_valid(),\n",
        "        eval_metrics={\n",
        "            'acc': tflearn.MetricSpec(\n",
        "                metric_fn=metrics.streaming_accuracy, prediction_key='class'\n",
        "            )\n",
        "        },\n",
        "        export_strategies=[saved_model_export_utils.make_export_strategy(\n",
        "            serving_input_fn,\n",
        "            default_output_alternative_key=None,\n",
        "            exports_to_keep=1\n",
        "        )],\n",
        "        train_steps = TRAIN_STEPS\n",
        "    )\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ndkbDuaJ559z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#split training data into train and evaluation data\n",
        "evaluation_data = train[15663:]\n",
        "training_data = train[:15663]\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BoLt5OBjHDaW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "evaluation_labels = evaluation_data['author']\n",
        "training_labels = training_data['author']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dGVigg_Lqn62",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "62fbc459-1a69-428a-e229-e5c6717735ec"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    train_results = cnn_model(training_data,training_labels, tf.contrib.learn.ModeKeys.TRAIN)\n",
        "    eval_results = cnn_model(evaluation_data,evaluation_labels, tf.contrib.learn.ModeKeys.EVAL)\n",
        "    init=tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "    "
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "words_sliced=SparseTensor(indices=Tensor(\"StringSplit_11:0\", shape=(?, 2), dtype=int64), values=Tensor(\"StringSplit_11:1\", shape=(?,), dtype=string), dense_shape=Tensor(\"StringSplit_11:2\", shape=(2,), dtype=int64))\n",
            "words_embed=Tensor(\"EmbedSequence_11/embedding_lookup/Identity:0\", shape=(15663, 200, 10), dtype=float32)\n",
            "words_conv=Tensor(\"Squeeze_16:0\", shape=(15663, 40), dtype=float32)\n",
            "logits=Tensor(\"fully_connected_10/BiasAdd:0\", shape=(15663, 3), dtype=float32)\n",
            "words_sliced=SparseTensor(indices=Tensor(\"StringSplit_12:0\", shape=(?, 2), dtype=int64), values=Tensor(\"StringSplit_12:1\", shape=(?,), dtype=string), dense_shape=Tensor(\"StringSplit_12:2\", shape=(2,), dtype=int64))\n",
            "words_embed=Tensor(\"EmbedSequence_12/embedding_lookup/Identity:0\", shape=(3916, 200, 10), dtype=float32)\n",
            "words_conv=Tensor(\"Squeeze_17:0\", shape=(3916, 40), dtype=float32)\n",
            "logits=Tensor(\"fully_connected_11/BiasAdd:0\", shape=(3916, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}